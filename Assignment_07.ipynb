{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 07.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1IdxFMvKkP62_TpizIPbyWn2qKCGgG0ZY",
      "authorship_tag": "ABX9TyN0QP12CJ5rsvjbvxPSsgyG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmfpdlxmtidl/MachineLearningAssignments/blob/master/Assignment_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a38QdfqvdMjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from math import *\n",
        "from decimal import Decimal\n",
        "\n",
        "scale = 0.1\n",
        "\n",
        "# Logistic regression\n",
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-scale * z))\n",
        "\n",
        "# g(x, y, theta)\n",
        "def g(x, y, theta):\n",
        "  sigma = 0\n",
        "  for i in range(10):\n",
        "    for j in range(10):\n",
        "      sigma +=  theta[i][j] * (x ** i) * (y ** j)\n",
        "  return sigma\n",
        "\n",
        "# Derivative of g(x, y, theta) by theta_ij\n",
        "def dg_dtheta_ij(x, y, i, j):\n",
        "  if(i == 0 and j ==0):\n",
        "    return 1\n",
        "  else:\n",
        "    return x ** i * y ** j\n",
        "\n",
        "# Objective Function\n",
        "def J(theta, data, lambda_):\n",
        "  sigma = 0\n",
        "  for x, y, l in data:\n",
        "    z = g(x, y, theta)\n",
        "    sigma += -int(l) * log(sigmoid(z)) - (1 - int(l)) * log(1 - sigmoid(z))\n",
        "\n",
        "  sigma2 = 0\n",
        "  for i in range(10):\n",
        "    for j in range(10):\n",
        "      sigma2 += theta[i][j] * theta[i][j]\n",
        "\n",
        "  return (1 / len(data)) * sigma + (lambda_ / 2) * sigma2\n",
        "\n",
        "# Derivative of Objective Function by theta_ij\n",
        "def dJ_dtheta_ij(theta, data, i, j, lambda_):\n",
        "  sigma = 0\n",
        "  for x, y, l in data:\n",
        "    z = g(x, y, theta)\n",
        "    sigma += (sigmoid(z) - int(l)) * dg_dtheta_ij(x, y, i, j)\n",
        "\n",
        "  return (scale / (len(data))) * sigma + lambda_ * theta[i][j]\n",
        "\n",
        "# determine whether theta is converged\n",
        "def is_converged(theta, next_theta):\n",
        "  for i in range(len(theta)):\n",
        "    for i in range(len(theta[0])):\n",
        "      if(Decimal(theta[i][j]) != Decimal(next_theta[i][j])):\n",
        "        return False\n",
        "  return True\n",
        "\n",
        "# the training accuracy is computed by (number of correct predictionstotal) / (number of predictions)\n",
        "def get_training_accuracy(theta, data):\n",
        "  correct = 0\n",
        "  for x, y, l in data:\n",
        "    if(g(x, y, theta) > 0 and int(l) == 1):\n",
        "      correct += 1\n",
        "    elif(g(x, y, theta) < 0 and int(l) == 0):\n",
        "      correct += 1\n",
        "    elif(g(x, y, theta) == 0):\n",
        "      correct += 1\n",
        "  return correct / len(data)\n",
        "\n",
        "# load the training data file ('data-nonlinear.txt')\n",
        "data = np.genfromtxt(\"drive/My Drive/Colab Notebooks/Machine Learning/Assignment 07/data-nonlinear.txt\", delimiter=',')\n",
        "\n",
        "# each row (x_i,y_i,l_i) of the data consists of a 2-dimensional point (x,y) with its label l\n",
        "# x,y ∈ R\n",
        "pointX  = data[:, 0]\n",
        "pointY  = data[:, 1]\n",
        "# l ∈ {0,1}\n",
        "label   = data[:, 2]\n",
        "\n",
        "pointX0 = pointX[label == 0]\n",
        "pointY0 = pointY[label == 0]\n",
        "\n",
        "pointX1 = pointX[label == 1]\n",
        "pointY1 = pointY[label == 1]\n",
        "\n",
        "# you can use any initial conditions θ = (θ0,θ1, ..., θk-1), for all k\n",
        "theta1 = np.ones((10, 10), float)\n",
        "theta2 = np.ones((10, 10), float)\n",
        "theta3 = np.ones((10, 10), float)\n",
        "\n",
        "# you should choose a learning rate α in such a way that the convergence is achieved\n",
        "learning_rate = 10\n",
        "\n",
        "lambda1 = 0.001\n",
        "lambda2 = 0.01\n",
        "lambda3 = 0.1\n",
        "\n",
        "# the i-th theta, training error J(θ0,θ1,θ2), and training accuracy\n",
        "theta1_i = [theta1[0][0]]\n",
        "theta2_i = [theta2[1][1]]\n",
        "theta3_i = [theta3[2][2]]\n",
        "J1_i = [J(theta1, data, lambda1)]\n",
        "J2_i = [J(theta2, data, lambda2)]\n",
        "J3_i = [J(theta3, data, lambda3)]\n",
        "training_accuracy1 = [get_training_accuracy(theta1, data)]\n",
        "training_accuracy2 = [get_training_accuracy(theta2, data)]\n",
        "training_accuracy3 = [get_training_accuracy(theta3, data)]\n",
        "\n",
        "steps = 30\n",
        "\n",
        "try:\n",
        "  while True:\n",
        "  #for step in range(steps):\n",
        "    # Gradient Descent\n",
        "    # find optimal parameters θ using the training data\n",
        "    next_theta1 = np.empty((10, 10), float)\n",
        "\n",
        "    # find optimal parameters θ using the training data\n",
        "    for i in range(len(theta1)):\n",
        "      for j in range(len(theta1[0])):\n",
        "        next_theta1[i][j] = theta1[i][j] - 70 * dJ_dtheta_ij(theta1, data, i, j, lambda1)\n",
        "\n",
        "    # break the loop if parameters are converged\n",
        "    if(is_converged(theta1, next_theta1)):\n",
        "      break\n",
        "\n",
        "    # save the estimated parameters (θ0​,θ1​,θ2​) at every iteration of gradient descent\n",
        "    theta1_i.append(theta1[0][0])\n",
        "    J1_i.append(J(theta1, data, lambda1))\n",
        "    training_accuracy1.append(get_training_accuracy(theta1, data))\n",
        "\n",
        "    # update the theta\n",
        "    theta1 = next_theta1\n",
        "\n",
        "  '''\n",
        "  while True:\n",
        "  #for step in range(steps):\n",
        "    # Gradient Descent\n",
        "    # find optimal parameters θ using the training data\n",
        "    next_theta2 = np.empty((10, 10), float)\n",
        "\n",
        "    # find optimal parameters θ using the training data\n",
        "    for i in range(len(theta2)):\n",
        "      for j in range(len(theta2[0])):\n",
        "        next_theta2[i][j] = theta2[i][j] - 5 * learning_rate * dJ_dtheta_ij(theta2, data, i, j, lambda2)\n",
        "\n",
        "    # break the loop if parameters are converged\n",
        "    if(is_converged(theta2, next_theta2)):\n",
        "      break\n",
        "\n",
        "    # save the estimated parameters (θ0​,θ1​,θ2​) at every iteration of gradient descent\n",
        "    theta2_i.append(theta2[0][0])\n",
        "    J2_i.append(J(theta2, data, lambda2))\n",
        "    training_accuracy2.append(get_training_accuracy(theta2, data))\n",
        "\n",
        "    # update the theta\n",
        "    theta2 = next_theta2\n",
        "  '''\n",
        "\n",
        "  #while True:\n",
        "  for step in range(steps):\n",
        "    # Gradient Descent\n",
        "    # find optimal parameters θ using the training data\n",
        "    next_theta3 = np.empty((10, 10), float)\n",
        "\n",
        "    # find optimal parameters θ using the training data\n",
        "    for i in range(len(theta3)):\n",
        "      for j in range(len(theta3[0])):\n",
        "        next_theta3[i][j] = theta3[i][j] - 16 * dJ_dtheta_ij(theta3, data, i, j, lambda3)\n",
        "\n",
        "    # break the loop if parameters are converged\n",
        "    if(is_converged(theta3, next_theta3)):\n",
        "      break\n",
        "\n",
        "    # save the estimated parameters (θ0​,θ1​,θ2​) at every iteration of gradient descent\n",
        "    theta3_i.append(theta3[0][0])\n",
        "    J3_i.append(J(theta3, data, lambda3))\n",
        "    training_accuracy3.append(get_training_accuracy(theta3, data))\n",
        "\n",
        "    # update the theta\n",
        "    theta3 = next_theta3\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  pass\n",
        "except ValueError:\n",
        "  pass\n",
        "\n",
        "# For adjusting hyper-parameters\n",
        "print(len(theta1_i))\n",
        "print(len(theta2_i))\n",
        "print(len(theta3_i))\n",
        "print('- Plot the estimated parameters θ at every iteration of gradient descent until convergence')\n",
        "print('- the colors for the parameters (θ1,θ2,θ3) should be red, green, blue, respectively')\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(range(len(theta1_i)), theta1_i, color='red')\n",
        "plt.plot(range(len(theta2_i)), theta2_i, color='green')\n",
        "plt.plot(range(len(theta3_i)), theta3_i, color='blue')\n",
        "plt.show()\n",
        "\n",
        "# 1. Plot the training data [1pt]\n",
        "print('1. Plot the training data points (x,y) with their labels l in colors (blue for label 0 and red for label 1)')\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(pointX0, pointY0, color='blue')\n",
        "plt.scatter(pointX1, pointY1, color='red')\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.show()\n",
        "\n",
        "# 2. Plot the training error with varying regularization parameters\n",
        "# the above three curves should be presented all together in a single figure\n",
        "print('2. Plot the training error  J(θ) at every iteration of gradient descent until convergence with varying regularization parameters')\n",
        "plt.figure(figsize=(8, 8))\n",
        "print('- choose a value for λ1​ in such a way that over-fitting is demonstrated (in red color) [3pt]')\n",
        "plt.plot(range(len(J1_i)), J1_i, color='red')\n",
        "print('- choose a value for λ2​ in such a way that just-right is demonstrated (in green color) [3pt]')\n",
        "plt.plot(range(len(J2_i)), J2_i, color='green')\n",
        "print('- choose a value for λ3​ in such a way that under-fitting is demonstrated (in blue color) [3pt]')\n",
        "plt.plot(range(len(J3_i)), J3_i, color='blue')\n",
        "plt.show()\n",
        "\n",
        "# 3. Display the values of the chosen regularization parameters\n",
        "print('3. Display the values of the chosen regularization parameters')\n",
        "print('- display the value of the chosen λ1​ for the demonstration of over-fitting (in red color) [1pt]')\n",
        "print('\\033[91m' + str(lambda1) + '\\033[0m')\n",
        "print('- display the value of the chosen λ2 for the demonstration of just-right (in green color) [1pt]')\n",
        "print('\\033[92m' + str(lambda2) + '\\033[0m')\n",
        "print('- display the value of the chosen λ3​ for the demonstration of under-fitting (in blue color) [1pt]')\n",
        "print('\\033[94m' + str(lambda3) + '\\033[0m')\n",
        "print()\n",
        "\n",
        "# 4. Plot the training accuracy with varying regularization parameters\n",
        "# the above three curves should be presented all together in a single figure\n",
        "print('4. Plot the training accuracy with varying regularization parameters')\n",
        "plt.figure(figsize=(8, 8))\n",
        "print('- plot the training accuracy with the chosen λ1​ for over-fitting (in red color) [3pt]')\n",
        "plt.plot(range(len(training_accuracy1)), training_accuracy1, color='red')\n",
        "print('- plot the training accuracy with the chosen λ2​ for just-right (in green color) [3pt]')\n",
        "plt.plot(range(len(training_accuracy2)), training_accuracy2, color='green')\n",
        "print('- plot the training accuracy with the chosen λ3​ for under-fitting (in blue color) [3pt]')\n",
        "plt.plot(range(len(training_accuracy3)), training_accuracy3, color='blue')\n",
        "plt.show()\n",
        "\n",
        "# 5. Display the final training accuracy with varying regularization parameters\n",
        "print('5. Display the final training accuracy with varying regularization parameters')\n",
        "print('- The final training accuracy obtained with the chosen λ1​ for over-fitting (in red color) [1pt]')\n",
        "print('\\033[91m' + str(training_accuracy1[-1] * 100) + '\\033[0m')\n",
        "print('- The final training accuracy obtained with the chosen λ2​ for just-right (in green color) [1pt]')\n",
        "print('\\033[92m' + str(training_accuracy2[-1] * 100) + '\\033[0m')\n",
        "print('- The final training accuracy obtained with the chosen λ3​ for under-fitting (in blue color) [1pt]')\n",
        "print('\\033[94m' + str(training_accuracy3[-1] * 100) + '\\033[0m')\n",
        "print()\n",
        "\n",
        "# 6. Plot the optimal classifier with varying regularization parameters superimposed on the training data\n",
        "print('6. Plot the optimal classifier with varying regularization parameters superimposed on the training data')\n",
        "plt.figure(figsize=(8, 8))\n",
        "y,x=np.ogrid[-1.5:1.5:100j,-1.5:1.5:100j]\n",
        "# the boundary of the classifier is defined by {(x,y) ∣ g(x,y;θ) = 0}\n",
        "print('- plot the boundary of the optimal classifier with the chosen λ1 for over-fitting at convergence (in red color) [3pt]')\n",
        "plt.contour(x.ravel(), y.ravel(), g(x, y, theta1), [0], colors='red')\n",
        "print('- plot the boundary of the optimal classifier with the chosen λ2 for just-right at convergence (in green color) [3pt]')\n",
        "plt.contour(x.ravel(), y.ravel(), g(x, y, theta2), [0], colors='green')\n",
        "print('- plot the boundary of the optimal classifier with the chosen λ3 for under-fitting at convergence (in blue color) [3pt]')\n",
        "plt.contour(x.ravel(), y.ravel(), g(x, y, theta3), [0], colors='blue')\n",
        "# plot the training data points (x,y) with their labels l\n",
        "# in colors superimposed on the illustration of the classifier (blue for label 0 and red for label 1)\n",
        "plt.scatter(pointX0, pointY0, color='blue')\n",
        "plt.scatter(pointX1, pointY1, color='red')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}