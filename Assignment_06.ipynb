{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 06.ipynb",
      "provenance": [],
      "mount_file_id": "1eGCO0XvGukRJfsTQGDBmsTipcuYgqYKK",
      "authorship_tag": "ABX9TyM8jJ/X+PJfNN8SHSXi2o7M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmfpdlxmtidl/MachineLearningAssignments/blob/master/Assignment_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2yslqDfuGxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from math import *\n",
        "from decimal import Decimal\n",
        "\n",
        "# Logistic regression\n",
        "def sigmoid(z):\n",
        "  return 1 / (1 + np.exp(-0.01 * z))\n",
        "\n",
        "def g(X, theta):\n",
        "  return theta[0] + theta[1] * (X[0] - 0.25) * (X[0] - 0.25) + theta[2] * (X[1] - 0.25) * (X[1] - 0.25)\n",
        "\n",
        "# Objective Function\n",
        "def J(theta, data):\n",
        "  sigma = 0\n",
        "  for i in data:\n",
        "    z = g(i, theta)\n",
        "    sigma += -int(i[2]) * log(sigmoid(z)) - (1 - int(i[2])) * log(1 - sigmoid(z))\n",
        "  return (1 / len(data)) * sigma\n",
        "\n",
        "# Derivative of Objective Function by theta_k\n",
        "def dJ_dtheta_k(theta, data, k):\n",
        "  sigma = 0\n",
        "  for i in data:\n",
        "    z = g(i, theta)\n",
        "    sigma += (sigmoid(z) - i[2]) * dg_dtheta_k(i, k)\n",
        "  return (1 / (100 * len(data))) * sigma\n",
        "\n",
        "# Derivative of g(x, y, theta) by theta_k\n",
        "def dg_dtheta_k(X, k):\n",
        "  if(k == 0):\n",
        "    return 1\n",
        "  else:\n",
        "    return X[k - 1] * X[k - 1]\n",
        "\n",
        "# determine whether theta is converged\n",
        "def is_converged(theta, next_theta):\n",
        "  for i in range(len(theta)):\n",
        "    if(Decimal(theta[i]) != Decimal(next_theta[i])):\n",
        "      return False\n",
        "  return True\n",
        "\n",
        "\n",
        "# load the training data file ('data-nonlinear.txt')\n",
        "data    = np.genfromtxt(\"drive/My Drive/Colab Notebooks/Machine Learning/Assignment 06/data-nonlinear.txt\", delimiter=',')\n",
        "\n",
        "# each row (x_i,y_i,l_i) of the data consists of a 2-dimensional point (x,y) with its label l\n",
        "# x,y ∈ R\n",
        "pointX  = data[:, 0]\n",
        "pointY  = data[:, 1]\n",
        "# l ∈ {0,1}\n",
        "label   = data[:, 2]\n",
        "\n",
        "pointX0 = pointX[label == 0]\n",
        "pointY0 = pointY[label == 0]\n",
        "\n",
        "pointX1 = pointX[label == 1]\n",
        "pointY1 = pointY[label == 1]\n",
        "\n",
        "# you can use any initial conditions (θ0,θ1,θ2), where θ0​,θ1​,θ2 ​∈ R\n",
        "theta = np.array([-1000, -500, 0], float)\n",
        "\n",
        "# you should choose a learning rate α in such a way that the convergence is achieved\n",
        "learning_rate = 1\n",
        "\n",
        "# the i-th training error J(θ0,θ1,θ2)\n",
        "theta0_i = [theta[0]]\n",
        "theta1_i = [theta[1]]\n",
        "theta2_i = [theta[2]]\n",
        "J_i = [J(theta, data)]\n",
        "training_accuracy = []\n",
        "\n",
        "#while True:\n",
        "for i in range(100000):\n",
        "  # Gradient Descent\n",
        "  next_theta = np.empty(3, float)\n",
        "  next_theta[0] = theta[0] - learning_rate * dJ_dtheta_k(theta, data, 0)\n",
        "  next_theta[1] = theta[1] - learning_rate * dJ_dtheta_k(theta, data, 1)\n",
        "  next_theta[2] = theta[2] - learning_rate * dJ_dtheta_k(theta, data, 2)\n",
        "\n",
        "  # break the loop if parameters are converged\n",
        "  if(is_converged(theta, next_theta)):\n",
        "    break\n",
        "\n",
        "  # save the estimated parameters (θ0​,θ1​,θ2​) at every iteration of gradient descent\n",
        "  theta0_i.append(theta[0])\n",
        "  theta1_i.append(theta[1])\n",
        "  theta2_i.append(theta[2])\n",
        "  J_i.append(J(theta, data))\n",
        "\n",
        "  # update the theta\n",
        "  theta = next_theta\n",
        "\n",
        "print(len(theta0_i) - 1)\n",
        "print('Theta:', theta)\n",
        "\n",
        "# 1. Plot the training data [1pt]\n",
        "print('1. Plot the training data points (x,y) with their labels l in colors')\n",
        "print('(blue for label 0 and red for label 1)')\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(pointX0, pointY0, color='blue')\n",
        "plt.scatter(pointX1, pointY1, color='red')\n",
        "plt.gca().set_aspect('equal', adjustable='box')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "x = range(len(theta0_i))\n",
        "plt.plot(x, theta0_i, color='red')\n",
        "plt.plot(x, theta1_i, color='green')\n",
        "plt.plot(x, theta2_i, color='blue')\n",
        "plt.show()\n",
        "\n",
        "# 3. Plot the training error [3pt]\n",
        "print('3. Plot the training error J(θ) at every iteration of gradient descent')\n",
        "print('until convergence (in blue color)')\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(x, J_i, color='blue')\n",
        "plt.show()\n",
        "\n",
        "# 4. Plot the training accuracy [3pt]\n",
        "print('4. Plot the training accuracy at every iteration of gradient descent')\n",
        "print('until convergence (in red color)')\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# 5. Write down the final training accuracy [5pt]\n",
        "# present the final training accuracy in number (%) at convergence\n",
        "\n",
        "\n",
        "# 6. Plot the optimal classifier superimposed on the training data [5pt]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jon43HO-xJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_oXzfI1xOPv",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1nzO1N4vqLU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "3cb9cb4a-5567-4adb-85eb-9e03a914f653"
      },
      "source": [
        "from timeit import timeit\n",
        "print(timeit('25000 ** i', setup='i = 2'))\n",
        "print(timeit('25000 * 25000'))\n",
        "print(timeit('math.pow(5, i)', setup='import math; i = 5'))\n",
        "\n",
        "print(sigmoid(36))\n",
        "print(data[0])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.23504268400029105\n",
            "0.01210025099953782\n",
            "0.18913895100013178\n",
            "0.9999999999999998\n",
            "[0.051267 0.69956  1.      ]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}